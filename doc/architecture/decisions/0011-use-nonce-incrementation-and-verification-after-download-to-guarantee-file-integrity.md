# 11. Use nonce-incrementation and verification after download to guarantee file integrity

Date: 2021

## Status

Accepted

Supercedes [10. Use nonce-incrementation to guarantee file integrity](0010-use-nonce-incrementation-to-guarantee-file-integrity.md)

## Context

The Crypt4GH format encrypts the files in blocks of 64 KiB, after which each data blocks unique nonce, ciphertext and MAC are saved to the c4gh file. This guarantees the integrity of the data blocks, however it does not guarantee the integrity of the entire file, and it is therefore possible that some blocks are rearranged, duplicated or missing, without the recipient knowing. Although we have chosen to not use the Crypt4GH format within the delivery system, we do use the same cipher (ChaCha20-Poly1305) and (since we cannot read huge files in memory) we have chosen to read the files in equally sized chunks. Therefore the integrity issue can potentially give problems for the delivery system.

## Decision

Perform the following file integrity checks within the DDS: 
* Automatically using Boto3 as described above
* A file checksum generated by Zstandard and stored in the compressed file
* By using nonce incrementation during the encryption process
* The MACs generated during encryption and verified during decryption

## Consequences

### File integrity check after upload and download
Initially, a file md5 checksum was generated during file processing. After upload, the checksum was compared to the ETag for the S3 object, indicating if the complete files had been uploaded. However, this method did not work for multipart uploads - files larger than 5 MB. On investigation of this, an GitHub issue was found, which addresses the issue: 

> ”[…] the md5 is only equal to the ETag under certain circumstances (i.e. non multipart upload).
Boto3 does not integrity check the md5 of the entire file for multipart uploads, but it does send an md5 header for each part that is uploaded when doing a multipart upload such that if there is an md5 mismatch in any of the parts, it will retry the request until it is correct.
That is probably the best we can do while still doing the transfer efficiently because determining the MD5 and doing integrity checking would require streaming the entire file upfront into memory.” ([link to issue]( https://github.com/boto/boto3/issues/845#issuecomment-253924586))
 
Due to this, no checksum verification is used during the upload. However, the file integrity is checked by the Data Delivery System in other ways, as described below.

### File integrity guarantee via compression/decompression
The Python Zstandard package used in the Data Delivery System allows the user to specify write_checksum = True which results in a checksum of the compressed data being saved in the compressed file. We have not yet determined the exact process of this (e.g. if it updates the checksum for each chunk resulting in one for the entire file or if it saves each chunks checksum), however this is under investigation. None the less, this (at the least) provides an integrity assurance of the 64 KiB chunks, since decompression will fail if the checksums do not match. This protects the data recipient from any corrupted (and possibly harmful) files. However, files which are already in a compressed format will not be compressed by the Data Delivery System. 

### File integrity guarantee via encryption/decryption
One solution to the integrity guarantee issue is to generate an additional checksum, representing the entire file, but for security reasons it is recommended to let the checksum generation be handled by (and in connection to) the encryption algorithm. Another solution is to generate a random nonce, and increment it for each block [25]. In this case the decryption only needs to know the first blocks nonce, and then increments it in the same way as the encryption until the end of the file. If the decryption of a block fails - the file has been altered. 

Since the nonces are regarded as public knowledge, the incrementation of the nonces do not decrease security - it may on the other hand increase the security: unique nonces for each data block is vital to the cryptographic security and generating a random nonce for each data block increases the risk of nonce reuse. Incrementing the nonces however mean that 2^96 (for 96 bit nonces as used in ChaCha20-Poly1305) data blocks can be encrypted before nonce reuse – 2^96×64×1024×8≈5.2 ×10^21  terrabytes (TB). This number may even be higher, since ChaCha20 itself adds and increments 4 bytes to the user-specified nonce. None the less, it’s safe to say that files of this size will not exist. An extra advantage to the nonce incrementation option is that only the first nonce needs to be saved, reducing the encryption overhead by 128 bits per data block. 

One problem with this solution is that, although we guarantee the order of the data blocks, we don’t know if all the data blocks are present – blocks at the end of the file may be missing without us knowing. Due to this, we also save the last nonce to the file. If decryption reaches the last nonce, the files integrity is proven, if not – something has gone wrong. 

Finally, as stated in the introduction of this section, the ChaCha20-Poly1305 (more specifically the Poly1305 authenticator) algorithm produces a message authentication code (MAC) during encryption of each chunk. This MAC is automatically saved to the file together with the ciphertext . When decryption is performed, each MAC is automatically authenticated by the Poly1305 prior to the decryption process. Thus, if a MAC validation fails (the file chunk is corrupted or the key is not correct), decryption is not performed. This process thereby provides confidentiality of the data, confirms its integrity and provides a guarantee for the data recipient that it is not exposed to any corrupted data.

Ref: ["Encrypting a set of related messages"](https://libsodium.gitbook.io/doc/secret-key_cryptography/encrypted-messages)

### Checksum verification after download, decryption and decompression

When processing the files, a checksum is automatically created representing the entire file, and saved in the database. During download, the user has the option of specifying the `--verify-checksum` option which verifies that the checksum generated after download, decryption and decompression is identical to the one saved in the database during upload. There is a small risk of hash collisions, however this risk is extremely small, specially with the checks explained above.  
